# Application Configuration
APP_NAME=Shuren Backend
DEBUG=False

# Database Configuration
# Format: postgresql+asyncpg://user:password@host:port/database
DATABASE_URL=postgresql+asyncpg://shuren_user:your_password@localhost:5432/shuren_db

# Local Database Password (for test_connection.py script)
DB_PASSWORD=your_password_here

# JWT Authentication
JWT_SECRET_KEY=your-secret-key-here-change-in-production
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_HOURS=24

# Google OAuth Configuration
GOOGLE_CLIENT_ID=your-google-client-id.apps.googleusercontent.com
GOOGLE_CLIENT_SECRET=your-google-client-secret

# Redis Configuration (for caching)
REDIS_URL=redis://localhost:6379/0

# ============================================================================
# LLM Configuration
# ============================================================================
# The LLM provider and model used for AI agent responses
# Supported providers: anthropic, openai, google
LLM_PROVIDER=anthropic

# Model name for the chosen provider
# Anthropic: claude-sonnet-4-5-20250929, claude-opus-4-5-20250929
# OpenAI: gpt-4-turbo, gpt-4, gpt-3.5-turbo
# Google: gemini-pro, gemini-1.5-pro
LLM_MODEL=claude-sonnet-4-5-20250929

# Temperature controls randomness (0.0 = deterministic, 1.0 = creative)
# Default: 0.7 for balanced responses
LLM_TEMPERATURE=0.7

# Maximum tokens in LLM response (affects response length and cost)
# Default: 4096 tokens (~3000 words)
LLM_MAX_TOKENS=4096

# ============================================================================
# LLM API Keys
# ============================================================================
# Provide the API key for your chosen LLM_PROVIDER
# Only the key matching your LLM_PROVIDER is required
# Get keys from:
# - Anthropic: https://console.anthropic.com/
# - OpenAI: https://platform.openai.com/api-keys
# - Google: https://makersuite.google.com/app/apikey

ANTHROPIC_API_KEY=your-anthropic-api-key-here
OPENAI_API_KEY=your-openai-api-key-here
GOOGLE_API_KEY=your-google-api-key-here

# ============================================================================
# Query Classifier Configuration
# ============================================================================
# Fast LLM model for routing queries to specialized agents
# Always uses Claude Haiku for speed (<100ms classification)
CLASSIFIER_MODEL=claude-haiku-4-5-20251001

# Low temperature for consistent classification decisions
CLASSIFIER_TEMPERATURE=0.1

# CORS Configuration (comma-separated origins)
CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# Logging
LOG_LEVEL=INFO

# ============================================================================
# LiveKit Configuration
# ============================================================================
# LiveKit server URL for real-time voice interactions
# Format: wss://your-project.livekit.cloud or ws://localhost:7880 for local
LIVEKIT_URL=wss://your-project.livekit.cloud

# LiveKit API credentials for room and token management
# Get these from: https://cloud.livekit.io/ (Cloud) or your self-hosted server
LIVEKIT_API_KEY=APIxxxxxxxxxxxxxx
LIVEKIT_API_SECRET=secretxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Number of idle agent workers to maintain for voice sessions
# Default: 2 workers ready to handle incoming voice sessions
LIVEKIT_WORKER_NUM_IDLE=2

# ============================================================================
# Voice Service Configuration
# ============================================================================
# Deepgram API key for Speech-to-Text in voice interactions
# Get your API key from: https://console.deepgram.com/
DEEPGRAM_API_KEY=your-deepgram-api-key-here

# Cartesia API key for Text-to-Speech in voice interactions
# Get your API key from: https://cartesia.ai/
CARTESIA_API_KEY=your-cartesia-api-key-here

# Voice agent context cache TTL in seconds (how long to cache user context)
# Default: 3600 seconds (1 hour)
VOICE_CONTEXT_CACHE_TTL=3600

# Maximum tokens for voice responses (keeps responses concise for voice)
# Default: 150 tokens (~100-120 words, ~30 seconds of speech)
VOICE_MAX_RESPONSE_TOKENS=150

# LLM provider for voice agent function calling
# Options: openai, anthropic, google
# Note: LiveKit Agents SDK has best support for OpenAI function calling
# Default: openai
VOICE_LLM_PROVIDER=openai
